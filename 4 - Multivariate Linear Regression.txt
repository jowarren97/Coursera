Multiple Features:
>n = #features (remember m is #training examples)
>x(i) = n-dim feature vector of ith training vector
>x(i)j = value of feature of j in ith training example
>NB 'i' is superscript while 'j' is subscript
>h(x) = theta0 + theta1.x1 + theta2.x2 + theta3.x3 ...

>define x0 = 1
>x = [x0, x1, x2, x3, ..., xn]
>theta = [theta0, theta1, theta2, theta3, ... thetan]
>h(x) = transpose(theta).x
>J(theta) = 1/2m * sum(i=1:m){[h(x(i))-y(i)]^2}
>thetaj := thetaj - a * d/dthetaj{J(theta)}
>thetaj := thetaj - a * 1/m * sum(i=1:m){[h(x(i))-y(i)]x(i)j}

Gradient Descent - Feature Scaling & Mean Normalisation:
>Descent can take a long time due to oscillation
>Occurs if bowl is skewed
>Perform feature scaling
>Can also make features have zero mean (replace xj with xj-muj)
>xj = (xj-muj)/xjrange

>For sufficiently small a, J(theta) should dec on every iteration
>a too large, J(theta) may not converge
>a too small, J(theta) converges slowly