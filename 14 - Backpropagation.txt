Backpropagation:
>L = #layers in network
>s_l = #units in layer l (not including bias unit)
>See slides for cost function
>Cost function has nested sums to account for:
>>multiple output nodes, which each contribute cost
>>additional theta terms due to >1 unit in network

Backpropagation Algorithm:
>Perform forward propagation:
>>a(1) = x; z(2) = theta(1)a(1); a(2) = g(z(2)); z(3) = theta(2)a(2); a(3) = g(z(3)) ...
>d(l)_j = error of node j in layer l
>e.g d(4)_j = a(4)_j - y_j  ...  this is the error of each output unit in layer 4
>where a(4)_j is h(x)_j  ...  jth element of h vector
>or d(4) = a(4) - y

>Now proceed to layer 3:
>d(3) = transpose(theta(3))*d(4) .* g'(z(3))  ...  where g'(z(3)) = a(3).*(1-a(3))
>d(2) = transpose(theta(2))*d(3) .* g'(z(2))  ...  N.B. g' is derivative not transpose
>No d(1) as first layer is inputs


>Set DELTA(l)_ij = 0
>for i = 1 to m
	>Set a(1) = x(i)
	>Perform fwd prop to compute a(l) for l = 2,3,...,L
	>Use y(i) to compute d(L) = a(L)-y(i)
	>Compute d(L-1), d(L-2), ..., d(2)
	>DELTA(l)_ij = DELTA(l)_ij + a(l)_j*d(l+1)_i
	>D(l)_ij = 1/m * DELTA(l)_ij  [ + lambda*theta(l)_ij   ]  <- if j != 0
	>N.B d/dtheta(l)_ij J(theta) = D(l)_ij


Gradient Checking:
>Use to check implementation of backprop
>Approximate gradient of J(theta) @ theta
>Use +/- e
>d/dtheta_i J(theta) ~ (J(theta_i+e, theta1, theta2, ...)-J(theta_i-e, theta ))/2e

Random Initialisation:
>Cant initialise all weights as zero
>Would make all a and d equal
>Also gradients all equal
>All weights change by same amount
>All units performing same function of inputs -> boring!
>Initialise theta to rand value [-e,e]


